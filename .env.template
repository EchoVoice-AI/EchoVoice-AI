# Copy to .env and fill values
FLASK_ENV=development
OPENAI_API_KEY=
VECTOR_DB_ENDPOINT=
VECTOR_DB_API_KEY=
DELIVERY_PROVIDER_API_KEY=
LOG_LEVEL=INFO
LANGSMITH_API_KEY = "sk_live_...your_key..."
# Also set your LLM keys if using OpenAI/Azure:
OPENAI_API_KEY = "sk_..."
# or for Azure:
AZURE_OPENAI_API_KEY = "..." 
AZURE_OPENAI_COMPLETION_DEPLOYMENT = "your-deployment-name"
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT = "your-emb-deployment"
# Optional: GitHub Models (via azure.ai.inference)
# Create a personal access token with `models:read` permission and set it here.
GITHUB_TOKEN=
# Embeddings model name (e.g. "openai/text-embedding-3-small")
# Use a distinct env var for embeddings to avoid confusion with LLM model names.
GITHUB_EMBEDDING_MODEL=
# Optional: LLM/Chat model name for GitHub-hosted completions
# Example: "gpt-4o-mini" or provider-specific model id
GITHUB_LLM_MODEL=
# Optional: override endpoint (default: https://models.github.ai/inference)
GITHUB_EMBEDDINGS_ENDPOINT=https://models.github.ai/inference