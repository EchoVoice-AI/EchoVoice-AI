import importlib

import pytest

# import the submodule explicitly (avoid `agent.graph` being shadowed by package attribute)
graph = importlib.import_module("agent.graph")


async def run_pipeline(initial_state: dict) -> dict:
    state = dict(initial_state)
    # run nodes sequentially and merge their outputs into state
    res = await graph.start_node(state, None)
    state.update(res or {})
    # Persist router decision explicitly via the goal_router_node
    # (the graph now expects routing to be performed by a dedicated node)
    try:
        res = await graph.goal_router_node(state, None)
        state.update(res or {})
    except AttributeError:
        # Fallback for environments that don't expose the node; allow
        # run_segmentation_node to handle routing for backwards-compatibility.
        pass
    # Run segmentation using the (now-persisted) router decision
    res = await graph.run_segmentation_node(state, None)
    state.update(res or {})
    res = await graph.priority_node(state, None)
    state.update(res or {})
    return state


def make_base_state(campaign_goal: str = "", user_message: str = "") -> dict:
    return {
        "campaign_goal": campaign_goal,
        "user_message": user_message,
        "final_segment": "",
        "confidence": 0.0,
        "segment_description": "",
        "context_query": None,
        "retrieved_content": [],
        "message_variants": [],
        "compliance_log": [],
        "winning_variant_id": None,
        "predicted_performance": {},
        "feedback_payload": {},
    }


@pytest.mark.asyncio
async def test_rfm_routing_and_priority():
    initial = make_base_state(campaign_goal="reduce_churn", user_message="")
    final = await run_pipeline(initial)
    assert "final_segment" in final
    assert final["final_segment"].lower().startswith("rfm:")
    assert final["confidence"] >= 0
    # Check raw segmentation structure and justification text
    raw = final.get("raw_segments") or {}
    rfm_entry = raw.get("rfm") or {}
    data = rfm_entry.get("raw_segmentation_data", {}) if isinstance(rfm_entry, dict) else {}
    assert "rfm" in data
    justification = data["rfm"].get("justification", "")
    # Accept either the original placeholder justification or the retention-specific text
    j = justification.lower()
    assert j != ""
    assert (
        "high engagement" in j
        or "basket" in j
        or "retention" in j
        or "retention candidate" in j
    )


@pytest.mark.asyncio
async def test_intent_routing_and_priority():
    initial = make_base_state(campaign_goal="increase_awareness", user_message="How do I return a product?")
    final = await run_pipeline(initial)
    assert "final_segment" in final
    assert final["final_segment"].lower().startswith("intent:")
    assert final["confidence"] >= 0
    raw = final.get("raw_segments") or {}
    intent_entry = raw.get("intent") or {}
    data = intent_entry.get("raw_segmentation_data", {}) if isinstance(intent_entry, dict) else {}
    assert "intent" in data
    justification = data["intent"].get("justification", "")
    assert "clarifying" in justification or "clarification" in justification


@pytest.mark.asyncio
async def test_behavioral_routing_and_priority():
    initial = make_base_state(campaign_goal="improve_engagement", user_message="I'm really frustrated that checkout is slow")
    final = await run_pipeline(initial)
    assert "final_segment" in final
    # allow either behavioral or intent depending on routing rules; ensure not empty
    assert final["final_segment"] != ""
    assert final["confidence"] >= 0
    raw = final.get("raw_segments") or {}
    behavioral_entry = raw.get("behavioral") or {}
    data = behavioral_entry.get("raw_segmentation_data", {}) if isinstance(behavioral_entry, dict) else {}
    assert "behavioral" in data
    justification = data["behavioral"].get("justification", "")
    assert "frustr" in justification or "emotional" in justification or justification != ""


@pytest.mark.asyncio
async def test_profile_fallback():
    initial = make_base_state(campaign_goal="general_outreach", user_message="")
    final = await run_pipeline(initial)
    assert "final_segment" in final
    assert final["final_segment"].lower().startswith("profile:")
    raw = final.get("raw_segments") or {}
    profile_entry = raw.get("profile") or {}
    data = profile_entry.get("raw_segmentation_data", {}) if isinstance(profile_entry, dict) else {}
    assert "profile" in data
    justification = data["profile"].get("justification", "")
    assert "default" in justification.lower() or "verbosity" in justification.lower() or justification != ""


@pytest.mark.asyncio
async def test_conflicting_signals():
    # campaign suggests retention (rfm) but user asks a question (intent)
    initial = make_base_state(campaign_goal="retention_offer", user_message="What's the price for the premium plan?")
    final = await run_pipeline(initial)
    assert "final_segment" in final
    # router policy determines winner; ensure we have a segment
    assert final["final_segment"] != ""


@pytest.mark.asyncio
async def test_purchase_keyword_triggers_rfm():
    initial = make_base_state(campaign_goal="seasonal_promo", user_message="I want to buy two of these â€” what's the price?")
    final = await run_pipeline(initial)
    assert "final_segment" in final
    # prefer rfm or intent depending on rule order; check that pipeline completed
    assert final["final_segment"] != ""
